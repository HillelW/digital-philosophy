{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Information and Behavioral Despair in the Talmud\n",
    "\n",
    "# The Problem\n",
    "\n",
    "R. Yitzchak is cited In B.T. Bava Metziah 21a as ruling that lost produce is considered \"scattered\" when it has a dispersal ratio of one kav per 4 cubits of area. The import of this categorization is that in such a situation, the owner is assumed to have despaired of ever recovering the produce, and keeping such produce is not classified as theft.\n",
    "\n",
    "Subsequently, an anonymous Talmudic voice ibid. asks that if the produce was found such that it was obviously placed by a human with intention, then the dispersion ratio is irrelevant. Alternatively, if the produce was found such that it was obviously not placed by a human with intention, then the dispersion ratio is once again irrelevant. \n",
    "\n",
    "## A Detour Through Algorithmic Information Theory \n",
    "\n",
    "Although the Talmud's distinction between intentional vs. unintentional placement makes *intuitive* sense, the Talmud does not offer a *precise* threshold between the two cases. Although we will not come to a conclusion on this matter here, we argue that the correct *framework* for this discussion is *algorithmic information theory*.\n",
    "\n",
    "The Talmud's distinction between intentional vs. unintentional placement is equivalent to the distinction between an arrangement which looks \"random\" vs. an arrangement which does not look \"random.\" That is, the Talmud assumes that one who places fruit intentionally will not allow the fruit to take assume an arrangement that looks \"random.\"\n",
    "\n",
    "We place \"random\" in quotes since it is not immediately clear how to precisely define a random arrangement in a mathematically rigorous way. Randomness could not be defined precisely until after the invention of the Universal Turing Machine by Alan Turing in 1936â€“1937.\n",
    "\n",
    "To see why probability theory is not sufficient to capture the intuitive notion of randomness, consider the following two 50-bit binary strings:\n",
    "\n",
    "1. **10101010101010101010101010101010101010101010101010**\n",
    "\n",
    "2. **00011101001000101101001000101111010100000100111101** \n",
    "\n",
    "Intuitively, it is obvious that the second bit string is \"more random\" than the first, since there is a *short pattern* which describes the first string, but no obvious pattern which describes the second string. In particular, the pattern which describes the first string is simply \"alternate between 1 and 0 twenty five times.\"\n",
    "\n",
    "However, if we restrict our attention to computing the *probability* of selecting each string from a random uniform probability distribution over a set of bit strings each of length 50, then probability theory tells us that each string has the *same* probability of being chosen: $2^{-50}$. If we want a measure that tells us the first string is *less random* than the second string, then we need to look outside of probability theory.\n",
    "\n",
    "In 1963, Andrey Kolmogorov introduced a new measure called *algorithimic complexity* which precisely captures the above intuition regarding randomness of bit strings.\n",
    "\n",
    "The *algorithmic complexity* of an object is the length of a shortest computer program (written in a universal programming language such as Python) that produces the object as output.\n",
    "\n",
    "An object is *algorithmically random* if and only if *every* computer program that can produce that object is at least as long as the string itself. \n",
    "\n",
    "To see how this definition cashes out, let's revisit the two bit strings from above and write python programs that compute these strings. One very short Python program that computes the first string is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10101010101010101010101010101010101010101010101010\n"
     ]
    }
   ],
   "source": [
    "print('10' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how *short* the above Python program is: only 16 characters (including the white space around the multiplication sign to make the program easier to read)! \n",
    "\n",
    "In contrast, because there is no obvious way of compressing the second string, the shortest Python program we can think of that reproduces the second string ends up *embedding the entire string inside of the program itself*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00011101001000101101001000101111010100000100111101\n"
     ]
    }
   ],
   "source": [
    "print('00011101001000101101001000101111010100000100111101')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two things:\n",
    "\n",
    "1. The length of the second program is much longer than the length of the first program - 59 characters\n",
    "2. The length of the second program is *longer* than the length of the second string!\n",
    "\n",
    "This gives us some intuition around how algorithmic complexity works: if the algorithmic complexity of an object is *less than* the length of the object in question, then the object is not fully random. Conversely, if the algorithmic complexity of an object is *greater than or equal* to the length of the object in question, then that object is *random*.\n",
    "\n",
    "One surprising fact about algorithmic complexity is that it is not computable! On its face, it seems like a waste of time to consider a measure that is not computable. However, it turns out that although we cannot obtain *exact* answers, the algorithmic information of an object can often be *approximated*. In particular, we can apply various known compression schemes (such as Huffman encoding) to the object and meausure how much shorter the compressed object is than the original object.\n",
    "\n",
    "We can use the *pybdm* library to see how such approximation schemes can be put to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithmic complexity: 28.610413747641715\n",
      "algorithmic complexity: 31.98545317898864\n",
      "algorithmic complexity: 35.31258643300726\n",
      "algorithmic complexity: 41.95701404545303\n",
      "algorithmic complexity: 150785.8568433854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pybdm import BDM\n",
    "\n",
    "# note that for a string with low randomness, such as a string\n",
    "# consisting of only the number 1, its algorithmic complexity \n",
    "# remains relatively constant even if we make the string arbitrarily long:\n",
    "bit_string = np.ones((100,), dtype=int)\n",
    "bdm = BDM(ndim=1)\n",
    "algorithmic_complexity = bdm.bdm(bit_string)\n",
    "print(f'algorithmic complexity: {algorithmic_complexity}')\n",
    "\n",
    "bit_string = np.ones((1000,), dtype=int)\n",
    "bdm = BDM(ndim=1)\n",
    "algorithmic_complexity = bdm.bdm(bit_string)\n",
    "print(f'algorithmic complexity: {algorithmic_complexity}')\n",
    "\n",
    "bit_string = np.ones((10000,), dtype=int)\n",
    "bdm = BDM(ndim=1)\n",
    "algorithmic_complexity = bdm.bdm(bit_string)\n",
    "print(f'algorithmic complexity: {algorithmic_complexity}')\n",
    "\n",
    "bit_string = np.ones((1000000,), dtype=int)\n",
    "bdm = BDM(ndim=1)\n",
    "algorithmic_complexity = bdm.bdm(bit_string)\n",
    "print(f'algorithmic complexity: {algorithmic_complexity}')\n",
    "\n",
    "# notice that the algorithmic complexity for a string with high\n",
    "# randomness is very large, despite the fact that this string\n",
    "# is the same length as the longest of the above bit strings\n",
    "bit_string = np.random.choice([0, 1], size=(1000000,))\n",
    "bdm = BDM(ndim=1)\n",
    "algorithmic_complexity = bdm.bdm(bit_string)\n",
    "print(f'algorithmic complexity: {algorithmic_complexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Return From the Detour\n",
    "\n",
    "Now that we understand the notion of algorithmic complexity and how it can be approximated, let us return to the Talmud's discussion of scattered fruit. \n",
    "\n",
    "At first blush, the above considerations may seem unrelated to the Talmud's discussion, since the Talmud deals with scattered fruit, not bit strings. However, the 20th century has shown that any discretized object (regardless of its dimensions) can be represented on a computer using an appropriate bit string. For example, digital audio, images and video all bottom-out as one-dimensional bit strings in the memory of some machine. \n",
    "\n",
    "Therefore, the Talmud's distinction between fruit which was placed intentionally (i,e, with a recognizable *shape* whose *description is short*) and fruit which was not placed intentionally (i.e. its arrangement appears random) is equivalent to the distinction between algorithmically *compressible* and *incompressible* strings.\n",
    "\n",
    "With this in mind, the following algorithm is appropriate in the case of one who finds scattered fruit:\n",
    "\n",
    "1. Take a digital photo of the scattered fruit.\n",
    "2. Approximate the algorithmic information of the fruit shown in the image (other details of the image can be ignored).\n",
    "3. If the approximate algorithmic information computed in step 2 *exceeds* some threshold, then the fruit may be kept without any concern of theft. Alternatively, if the algorithmic information computed in step 2 is *less than or equal* to some threshold, then the fruit must be left as it is.\n",
    "\n",
    "The normative threshold above which the algorithmic information associated with multiple lost objects allows a finder to take possession of the objects remains to be decided by a future Talmudic scholar.\n",
    "\n",
    "Although the Talmud happens to discuss fruit, the general principle of using algorithmic information as a way of distinguishing between cases of behavioral despair and cases where no behavioral despair has taken place can be generalized to many other objects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
